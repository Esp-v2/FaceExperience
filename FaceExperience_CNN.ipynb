{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "FaceExperience_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq9hvgg8zQ2p"
      },
      "source": [
        "**CNNによる2次元顔画像表情判定**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlyygK5u_LGY",
        "outputId": "b5ca9878-07b9-4515-e9c3-a2c7f7a65374"
      },
      "source": [
        "!ps aux\n",
        "!kill -9 <pid>"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n",
            "root           1  0.0  0.0    992     4 ?        Ss   05:23   0:00 /sbin/docker-\n",
            "root           7  0.0  0.3 337252 47092 ?        Sl   05:23   0:00 /tools/node/b\n",
            "root          18  0.0  0.0  35892  4824 ?        Ss   05:23   0:00 tail -n +0 -F\n",
            "root          41  0.0  0.3 160356 41748 ?        S    05:23   0:00 python3 /usr/\n",
            "root          54  0.1  0.4 193892 60064 ?        Sl   05:23   0:01 /usr/bin/pyth\n",
            "root          55  0.0  0.0 706824  5292 ?        Sl   05:23   0:00 /usr/local/bi\n",
            "root          66 36.0  0.8 697664 115320 ?       Ssl  05:37   0:01 /usr/bin/pyth\n",
            "root          86  1.0  0.1  93576 14320 ?        Sl   05:37   0:00 /usr/bin/pyth\n",
            "root         103  0.0  0.0  59036  6364 ?        R    05:37   0:00 ps aux\n",
            "/bin/bash: -c: line 0: syntax error near unexpected token `newline'\n",
            "/bin/bash: -c: line 0: `kill -9 <pid>'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNFBa-nYFNkR",
        "outputId": "534bab48-333c-4700-ebd4-aa034974af64"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XKF13tnERHW"
      },
      "source": [
        "import os\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, Dropout, Activation, Flatten,MaxPooling2D,Conv2D\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emmlcpStzibv"
      },
      "source": [
        "**訓練画像、検証画像、テスト画像のディレクトリ**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4YNFRZ-FHHp"
      },
      "source": [
        "classes = ['angry', 'disgust','fear','happy','neutral','sad','suprise']\n",
        "nb_classes = len(classes)\n",
        "#batch_size_for_data_generator = 20\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/kaggle/Facial Experience'\n",
        "\n",
        "train_dir = base_dir+'/train'\n",
        "validation_dir = base_dir+'/valid'\n",
        "test_dir = base_dir+'/test'\n",
        "\n",
        "img_rows, img_cols = 48, 48"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1DoKb8vJULr"
      },
      "source": [
        "os.path.join(base_dir,'path')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3U9Oi0z0Ob9"
      },
      "source": [
        "**ImageDataGeneratorを使って画像データを拡張する**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZWcxF46V24h",
        "outputId": "f18db535-898f-4627-bd02-d547e99b302a"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "train_generator = train_datagen.flow_from_directory(directory=train_dir,\n",
        "                                                    target_size=(img_rows, img_cols),\n",
        "                                                    color_mode='rgb',\n",
        "                                                    classes=classes,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    batch_size=1117,\n",
        "                                                    shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 84892 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuGxBj_f1yPP"
      },
      "source": [
        "train 84892枚\n",
        "2x2x19x1117\n",
        "バッジサイズ2048"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRDqyGVIWFjm",
        "outputId": "b8a527ce-b0c2-44db-dbd5-836a2be34dae"
      },
      "source": [
        "valid_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "validation_generator = valid_datagen.flow_from_directory(directory=validation_dir,\n",
        "                                                        target_size=(img_rows, img_cols),\n",
        "                                                        color_mode='rgb',\n",
        "                                                        classes=classes,\n",
        "                                                        class_mode='categorical',\n",
        "                                                        batch_size=131,\n",
        "                                                        shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 21615 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7EkjT3x2bk5"
      },
      "source": [
        "valid 21615枚 3x5x11x131 バッジサイズ2048"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLSUpDGs2ifm"
      },
      "source": [
        "**CNNモデル**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSQuX9v64IAT"
      },
      "source": [
        "なんで512?→特に理由なし1024でもなんでもいい\n",
        "\n",
        "dropoutの役割→層から層への伝達の際に出力データを意図的に0とする事で、訓練データに過剰適合する問題を回避する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIg6Ng3d52cL"
      },
      "source": [
        "出力層\n",
        "二値分類→sigmoid\n",
        "他クラス分類→softmax\n",
        "回帰分類→恒等関数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3XQUcKsfq5x"
      },
      "source": [
        "leaky relu\n",
        "\n",
        "relu関数においてx<0のとき正の傾きを与える"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkvDQZF0n4gY"
      },
      "source": [
        "過学習を起こしたら、入力層にDropout率0.2、隠れ層に0.5を適用する\n",
        "\n",
        "https://deepage.net/deep_learning/2016/10/17/deeplearning_dropout.html#tflearn%E3%81%AE%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQPRXwdarKaH",
        "outputId": "c966e6d7-098d-4d54-ca4a-e122320adc68"
      },
      "source": [
        "from keras.layers import BatchNormalization\n",
        "#first layer\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), input_shape = (img_rows, img_cols, 3) ,padding='same'))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "#second layer\n",
        "model.add(Conv2D(32, (3,3), padding='same' ))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "#regularization\n",
        "model.add(MaxPooling2D(pool_size = (2,2), strides=(2,2)))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "#thirt layer\n",
        "model.add(Conv2D(64, (3,3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "#fourth layer\n",
        "model.add(Conv2D(64, (3,3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides= (2,2)))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "#classification layer\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(nb_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 48, 48, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 48, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 48, 48, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 48, 48, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 48, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 24, 24, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               4719104   \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 7)                 3591      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 7)                 0         \n",
            "=================================================================\n",
            "Total params: 4,791,079\n",
            "Trainable params: 4,789,671\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szvIOUtftc2R"
      },
      "source": [
        "miniVGGnet\n",
        "https://github.com/matvi/miniVGGNet/blob/master/cnn/MiniVGGNet.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH0jxO1oWNjC"
      },
      "source": [
        "# model=Sequential()\n",
        "# # 畳み込み層\n",
        "# model.add(Conv2D(32, (3, 3), activation='relu',input_shape=(img_rows, img_cols, 3)))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# model.add(Dropout(0.5))\n",
        "\n",
        "# model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# model.add(Dropout(0.5))\n",
        "\n",
        "# model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# model.add(Dropout(0.5))\n",
        "\n",
        "# # 全結合層\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# # 出力層\n",
        "# model.add(Dense(nb_classes, activation='softmax'))\n",
        "          \n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j8jbWX67fgr"
      },
      "source": [
        "モデル参照先\n",
        "https://sciresol.s3.us-east-2.amazonaws.com/IJST/Articles/2019/Issue-24/Article9.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJb_lDZoWX96",
        "outputId": "830d09a3-392c-4a68-bf9e-d447a3286b6b"
      },
      "source": [
        "opt = Adam(lr=0.001, decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy',optimizer= opt, metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCQQwsls4aKW"
      },
      "source": [
        "**コールバックを使った学習**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuTqNiABCVar"
      },
      "source": [
        "hdf5_file = os.path.join(base_dir, 'model.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKIS8wpnzFBa"
      },
      "source": [
        "再度ここから"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aHscPhIUxnW"
      },
      "source": [
        "model.load_weights(hdf5_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZy9RFVmBks9",
        "outputId": "b1fb9655-1546-4eaa-d480-718e2df93245"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "modelCheckpoint = ModelCheckpoint(filepath = hdf5_file,\n",
        "                                  monitor='loss',\n",
        "                                  verbose=1,\n",
        "                                  save_best_only=True,\n",
        "                                  save_weights_only=False,\n",
        "                                  mode='min',\n",
        "                                  period=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "id": "CwpOn_MdWq91",
        "outputId": "bcf712bc-051b-48e7-e79e-2087c16727be"
      },
      "source": [
        "history = model.fit(train_generator,\n",
        "                    steps_per_epoch=76,\n",
        "                    epochs=5,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=165,\n",
        "                    callbacks=[modelCheckpoint],\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "76/76 [==============================] - 20080s 264s/step - loss: 0.5309 - acc: 0.8128 - val_loss: 1.6786 - val_acc: 0.4953\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.53085, saving model to /content/drive/MyDrive/kaggle/Facial Experience/model.hdf5\n",
            "Epoch 2/5\n",
            "76/76 [==============================] - 1319s 17s/step - loss: 0.2997 - acc: 0.9047 - val_loss: 1.8712 - val_acc: 0.4661\n",
            "\n",
            "Epoch 00002: loss improved from 0.53085 to 0.29968, saving model to /content/drive/MyDrive/kaggle/Facial Experience/model.hdf5\n",
            "Epoch 3/5\n",
            "76/76 [==============================] - 1366s 18s/step - loss: 0.1635 - acc: 0.9551 - val_loss: 2.1309 - val_acc: 0.4564\n",
            "\n",
            "Epoch 00003: loss improved from 0.29968 to 0.16354, saving model to /content/drive/MyDrive/kaggle/Facial Experience/model.hdf5\n",
            "Epoch 4/5\n",
            "12/76 [===>..........................] - ETA: 17:05 - loss: 0.0927 - acc: 0.9819"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9f7e0b6fda57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m165\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodelCheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     verbose=1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RWIBKMEWxTo"
      },
      "source": [
        "model.save(hdf5_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB1RGpShZAxx"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT-olribZELf"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILeMIAeoZE5L"
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io1A1fx02E3p"
      },
      "source": [
        "**テスト**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcPSD3HajYzT"
      },
      "source": [
        "#test_datagen = ImageDataGenerator(rescale=1.0 / 255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mo7v1tA2IDM"
      },
      "source": [
        "#model=keras.models.load_model(hdf5_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbJwdZZoZJqz"
      },
      "source": [
        "# import numpy as np\n",
        "# test_generator = test_datagen.flow_from_directory(directory=test_dir,\n",
        "#                                                   target_size=(img_rows, img_cols),\n",
        "#                                                   color_mode='rgb',\n",
        "#                                                   classes=classes,\n",
        "#                                                   class_mode='categorical',\n",
        "#                                                   batch_size=32,\n",
        "#                                                   shuffle=False)\n",
        "\n",
        "# test_steps_per_Epoch = np.math.ceil(test_generator.samples / test_generator.batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eta-fthmqFhI"
      },
      "source": [
        "# predictions = model.predict_generator(test_generator, steps=test_steps_per_Epoch)\n",
        "# # Get most likely class\n",
        "# predicted_classes = np.argmax(predictions, axis=1)\n",
        "# # Ground-Truthクラスとクラスラベルを取得する\n",
        "# true_classes = test_generator.classes\n",
        "# class_labels = list(test_generator.class_indices.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c3S0sPF3z0W"
      },
      "source": [
        "適合率（precision）: precision_score()：実際に正/正と予測\n",
        "\n",
        "再現率（recall）: recall_score()：正と予測/実際に正\n",
        "\n",
        "F1値（F1-measure）: f1_score()：0-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9ysbq3h8c_B"
      },
      "source": [
        "# from sklearn import metrics\n",
        "# # scikit-learnを使用して統計を取得する\n",
        "# report = metrics.classification_report(true_classes, \n",
        "#                                        predicted_classes, \n",
        "#                                        labels=np.arange(len(classes)),\n",
        "#                                        target_names=classes)\n",
        "# print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k-jUgsC5b3Y"
      },
      "source": [
        "**参考**\n",
        "\n",
        "\n",
        "*   [Keras / CNN] 多クラス画像分類 --- ラーメンの味分類\n",
        "\n",
        "    https://qiita.com/Phoeboooo/items/cfe8560fe8a285855340\n"
      ]
    }
  ]
}